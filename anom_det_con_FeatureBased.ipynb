{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method1 : using  sentiment and length Features \n",
    "Imagine a platform that processes a continuous stream of conversations from a \n",
    "generative agent (StoryBot). To maintain a safe and engaging environment, you need to monitor these conversations in real time for anomalies such as sudden shifts in sentiment, unusual topic spikes, or atypical user behavior that could indicate emerging issues or opportunities. \n",
    "**Task**\n",
    "-  Develop a streaming machine learning pipeline in Python that ingests data (message \n",
    "and metadata), does feature extraction and preprocessing in near real time.\n",
    "-   applies an anomaly detection model (your own or from a 3rd party), and returns alerts when anomalies are detected. \n",
    "- For this task, feel free to pick an anomaly you think would be interesting to track (e.g., changes in mood, change in emoji use, shifts in language, use of the phrase “I don’t know”,  prompt injection attacks, etc.).\n",
    "- Provide a brief README that outlines the architecture, setup instructions, and how to run tests.   \n",
    "The datasets include:\n",
    "\n",
    "**Conversations** - One-on-one conversations between users and an AI agent (StoryBot)\n",
    "### conversations.json\n",
    "\n",
    "Contains conversations between individual users and StoryBot. Each conversation includes:\n",
    "\n",
    "- `messages_list`: List of messages in the conversation\n",
    "- `ref_conversation_id`: Unique identifier for the conversation\n",
    "- `ref_user_id`: ID of the user participating in the conversation\n",
    "\n",
    "Example structure:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"messages_list\": [\n",
    "    {\"message\": \"Good morning! How are you feeling today?\",\n",
    "    \"ref_conversation_id\": 42615,\n",
    "    \"ref_user_id\": 1,\n",
    "    \"transaction_datetime_utc\":  \"2023-10-01T08:00:00Z\",\n",
    "    \"screen_name\": \"StoryBot\"\n",
    "    },\n",
    "    {\"message\": \"I'm doing well, thanks for asking! Just trying to get through the day.\",\n",
    "    \"ref_conversation_id\": 42615,\n",
    "    \"ref_user_id\": 822,\n",
    "    \"transaction_datetime_utc\":  \"2023-10-01T08:01:00Z\",\n",
    "    \"screen_name\": \"User822\"\n",
    "    },\n",
    "    {\"message\": \"That's great to hear! Is there anything specific on your mind?\",\n",
    "    \"ref_conversation_id\": 42615,\n",
    "    \"ref_user_id\": 1,\n",
    "    \"transaction_datetime_utc\":  \"2023-10-01T08:02:00Z\",\n",
    "    \"screen_name\": \"StoryBot\"\n",
    "    },\n",
    "  ],\n",
    "  \"ref_conversation_id\": 42615,\n",
    "  \"ref_user_id\": 822\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libarries\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read JSON File to Load Conversations\n",
    "\n",
    "- Define the folder and file paths for the dataset.\n",
    "- Prepare to load the `conversations.json` file from the `data` directory within the current working directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get folder and filepaths\n",
    "#\n",
    "curr_folder = os.getcwd()\n",
    "conver_json = \"conversations.json\"\n",
    "json_file = os.path.join(curr_folder,\"data\",conver_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Conversations from JSON\n",
    "\n",
    "- Define a function `read_json()` to load conversation data from the specified JSON file.\n",
    "    - Handles file not found and JSON decoding errors gracefully.\n",
    "- Read the raw conversation data into memory.\n",
    "- Count and print the total number of conversations loaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file_path):\n",
    "    \"\"\"\n",
    "    Loads conversations from the conversations.json file.\n",
    "    Args:\n",
    "        file_path (str): The path to the JSON file.\n",
    "    Returns:\n",
    "        list: A list of conversation objects, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_file, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found: {file_path}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Could not decode JSON from the file {file_path}.\")\n",
    "        return None\n",
    "# read RAW JSON data\n",
    "raw_conversations = read_json(json_file)\n",
    "total_conv = len(raw_conversations)\n",
    "print(f\"number of conversations is {total_conv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split Data into Training and Test Sets\n",
    "\n",
    "- Use `train_test_split` from scikit-learn to divide the loaded conversations into training and test sets.\n",
    "- Set a random seed for reproducibility.\n",
    "- Define the percentage split between training and testing (default: 80% train, 20% test).\n",
    "- If the dataset is very small, warn the user and suggest using a K-fold strategy.\n",
    "- Print the number of conversations in each split.  \n",
    "\n",
    "**NOTE**: Alternatively, this could all be training set, and we test on a different set of conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # For splitting data\n",
    "import random\n",
    "\n",
    "# Assuming StoryBot's ID is 1, based on given json file \n",
    "STORYBOT_USER_ID = 1 \n",
    "\n",
    "RANDOM_SEED = 42 # For reproducible splits\n",
    "train_per = 80\n",
    "test_per = 100 - train_per\n",
    "min_conv_to_train = 8\n",
    "if total_conv > 0:\n",
    "    # Too small a data to train\n",
    "    if total_conv < min_conv_to_train: \n",
    "        print(\"Warning: Very few conversations. Use K-fold strategy to train and test\")        \n",
    "    else:\n",
    "        train_conversations, test_conversations = train_test_split(\n",
    "            raw_conversations, \n",
    "            test_size= test_per/100,       \n",
    "            random_state=RANDOM_SEED # For reproducibility\n",
    "        )\n",
    "\n",
    "    print(f\"\\nNumber of conversations for training: {len(train_conversations)}\")\n",
    "    print(f\"Number of conversations for testing: {len(test_conversations)}\")   \n",
    "else:\n",
    "    print(\"Cannot proceed with splitting as no conversations were loaded.\")\n",
    "    train_conversations, test_conversations = [], [] # Initialize as empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Function to Extract User Message Details\n",
    "\n",
    "- Define `extract_user_message_details()` to:\n",
    "    - Iterate through each conversation and its messages.\n",
    "    - Extract details for each message not sent by StoryBot, including:\n",
    "        - Conversation ID\n",
    "        - User ID\n",
    "        - Screen name\n",
    "        - Timestamp (parsed as datetime)\n",
    "        - Message text\n",
    "    - Handle malformed message data gracefully.\n",
    "- Returns a list of dictionaries, each representing a user message with relevant metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_user_message_details(list_of_conversations, is_storybot_user_id):\n",
    "    \"\"\"\n",
    "    Extracts message details (text, conv_id, timestamp) for all messages for all users\n",
    "    NOT sent by the is_storybot_user_id.\n",
    "\n",
    "    Args:\n",
    "        list_of_conversations (list): A list of conversation objects.\n",
    "        is_storybot_user_id (int): The user ID of StoryBot.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dict contains \n",
    "              'conversation_id', 'original_user_id', 'screen_name', \n",
    "              'timestamp', and 'message_text' for a user message.\n",
    "    \"\"\"\n",
    "    user_messages_details = []\n",
    "    # for each conversation with  a user\n",
    "    for conversation in list_of_conversations:\n",
    "        # exgtract conversation id\n",
    "        conv_id = conversation.get('ref_conversation_id', 'unknown_conv_id')\n",
    "        # extract list of  messages\n",
    "        messages_list = conversation.get('messages_list', [])\n",
    "        # build new list with updated datetime\n",
    "        for message_data in messages_list:\n",
    "            if not isinstance(message_data, dict): # Basic check for message structure\n",
    "                # print(f\"Skipping malformed message in conv {conv_id}\")\n",
    "                continue\n",
    "\n",
    "            message_user_id = message_data.get('ref_user_id')\n",
    "            \n",
    "            if message_user_id is not None and message_user_id != is_storybot_user_id:\n",
    "                try:\n",
    "                    timestamp_str = message_data.get('transaction_datetime_utc')\n",
    "                    parsed_timestamp = None\n",
    "                    if timestamp_str:\n",
    "                        parsed_timestamp = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))\n",
    "                    \n",
    "                    user_messages_details.append({\n",
    "                        'conversation_id': conv_id,\n",
    "                        'original_user_id': message_user_id,\n",
    "                        'screen_name': message_data.get('screen_name', 'UnknownUser'),\n",
    "                        'timestamp': parsed_timestamp,\n",
    "                        'message_text': message_data.get('message', '') # Ensure text is a string\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing a user message in conv {conv_id}: {e}. Message data: {message_data}\")\n",
    "\n",
    "    return user_messages_details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract Message Data for Training and Test\n",
    "\n",
    "- Use the previously defined function to extract user messages from both training and test sets.\n",
    "- Print the number of user messages extracted for each set.\n",
    "- Display an example message for sanity checking.\n",
    "- These extracted messages will be used for embedding and downstream anomaly detection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract user messages for training\n",
    "train_user_message_details = []\n",
    "if train_conversations:\n",
    "    # create list of all training user data , with updated time\n",
    "    train_user_message_details = extract_user_message_details(train_conversations, STORYBOT_USER_ID)\n",
    "    print(f\"\\nExtracted {len(train_user_message_details)} user messages for training.\")\n",
    "    # print first  user message for sanity check\n",
    "    if train_user_message_details:\n",
    "        print(f\"Example training user message detail: {train_user_message_details[0]}\")\n",
    "\n",
    "# create list of all test user data , with updated time\n",
    "test_user_message_details = []\n",
    "if test_conversations:\n",
    "    test_user_message_details = extract_user_message_details(test_conversations, STORYBOT_USER_ID)\n",
    "    print(f\"Extracted {len(test_user_message_details)} user messages for testing.\")\n",
    "    if test_user_message_details:\n",
    "        print(f\"Example testing user message detail: {test_user_message_details[0]}\")\n",
    "\n",
    "# exytract  'message_text' from these lists.\n",
    "\n",
    "# train_user_texts = [detail['message_text'] for detail in train_user_message_details]\n",
    "# test_user_texts = [detail['message_text'] for detail in test_user_message_details]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unlike in clustering where i needed message data to create embeddings, here\n",
    "- For feature-based anomaly detection, we use the extracted user message details rather than message embeddings.\n",
    "- Key features required:\n",
    "    - `message_text` for sentiment analysis and length calculation.\n",
    "    - `conversation_id` and `timestamp` to calculate features like sentiment shift within the same conversation.\n",
    " **we could also do compariosn with history of sentiments**\n",
    " -  **possible issue**: In 281 messages,  lets say first user had all psoiutive messages (5). second user starts  with allnegative (4), . then, when we move from index 4 to 5, wouldnt we flag  sentoment shift as an anomaly, while it shouldn't.\n",
    " -  **solution** The groupby('conversation_id') is  ensures that the .diff() operation is applied only to messages within the same conversation. \n",
    "    So, it will not calculate a shift between the last message of conversation A and the first message of conversation B. This is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Engineering: Sentiment and Length\n",
    "\n",
    "- Use VADER sentiment analyzer to compute sentiment scores for each user message.\n",
    "- Calculate the length of each message.\n",
    "- Compute the sentiment shift for each user within a conversation by comparing the sentiment score to the previous message from the same user in the same conversation.\n",
    "- Fill missing sentiment shift values (first message in a conversation) with zero.\n",
    "- Return the enhanced message data as a list of dictionaries with new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd \n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def engineer_features_for_messages(message_details_list):\n",
    "    \"\"\"\n",
    "    Adds sentiment score and message length to each message detail.\n",
    "    Focuses on user messages for sentiment shift.\n",
    "    \"\"\"\n",
    "    if not message_details_list:\n",
    "        return []\n",
    "\n",
    "    # Create a DataFrame for easier processing if not already\n",
    "    df = pd.DataFrame(message_details_list)    \n",
    "\n",
    "    #Sentiment Score for all messages\n",
    "    # Ensure 'message_text' is string, handle potential NaN if any step before failed\n",
    "    df['sentiment_compound'] = df['message_text'].astype(str).apply(lambda text: analyzer.polarity_scores(text)['compound'])\n",
    "\n",
    "    # 2. Message Length for all messages\n",
    "    df['message_length'] = df['message_text'].astype(str).apply(len)\n",
    "    \n",
    "    # 3. User Sentiment Shift (within each conversation)\n",
    "    # We only care about this for user messages.\n",
    "    # Initialize\n",
    "    df['user_sentiment_shift'] = np.nan \n",
    "    \n",
    "    # Add original_user_id to the df if it's not already there, ensuring it's numeric\n",
    "    if 'original_user_id' not in df.columns and 'user_id' in df.columns: \n",
    "        df['original_user_id'] = df['user_id']\n",
    "    \n",
    "    # Ensure original_user_id is numeric, coercing errors\n",
    "    df['original_user_id'] = pd.to_numeric(df['original_user_id'], errors='coerce')\n",
    "\n",
    "    \n",
    "    for conv_id, group in df.groupby('conversation_id'):\n",
    "        #  Sort these user messages chronologically by their 'timestamp'.\n",
    "        user_messages_group = group[group['original_user_id'] != STORYBOT_USER_ID].sort_values(by='timestamp')\n",
    "        if not user_messages_group.empty:\n",
    "            # Calculate shift from previous user message in the same conversation\n",
    "            shifts = user_messages_group['sentiment_compound'].diff()\n",
    "            # Assign these calculated 'shifts' back to the correct rows (user messages)\n",
    "            #    in the main DataFrame 'df' using their original index.\n",
    "            df.loc[user_messages_group.index, 'user_sentiment_shift'] = shifts\n",
    "            \n",
    "    # Fill NaN shifts (e.g., for the first user message in a convo) with 0 \n",
    "    df['user_sentiment_shift'].fillna(0, inplace=True)\n",
    "\n",
    "    return df.to_dict('records') \n",
    "\n",
    "# --- Apply feature engineering ---\n",
    "# Assuming train_user_message_details and test_user_message_details are lists of dicts\n",
    "\n",
    "# Add features to training user messages\n",
    "print(\"\\n--- Engineering features for training user messages ---\")\n",
    "train_messages_with_features = engineer_features_for_messages(train_user_message_details)\n",
    "if train_messages_with_features:\n",
    "    print(f\"Added features to {len(train_messages_with_features)} training user messages.\")\n",
    "    # print(pd.DataFrame(train_messages_with_features).head()) # Optional: view\n",
    "else:\n",
    "    print(\"No training user messages to engineer features for.\")\n",
    "\n",
    "print(\"\\n--- Engineering features for testing user messages ---\")\n",
    "test_messages_with_features = engineer_features_for_messages(test_user_message_details)\n",
    "if test_messages_with_features:\n",
    "    print(f\"Added features to {len(test_messages_with_features)} testing user messages.\")\n",
    "    # print(pd.DataFrame(test_messages_with_features).head()) # Optional: view\n",
    "else:\n",
    "    print(\"No testing user messages to engineer features for.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Flag Anomalies in Test User Messages\n",
    "\n",
    "- For each test user message, add anomaly flags:\n",
    "    - `isAnomalousTooShort`: Set to 1 if the message is shorter than the calculated threshold.\n",
    "    - `isAnomalousSentimentFlip`: Set to 1 if the user's sentiment flips from positive to negative or vice versa compared to their previous message in the same conversation.\n",
    "- Track the last sentiment category for each user within each conversation to enable accurate flip detection.\n",
    "- Store all processed messages and their anomaly flags in a list for further analysis.\n",
    "- Print the number of processed messages and show an example of the flagged data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len_thr_shrt = None\n",
    "# VADER score >= 0.05 is considered positive\n",
    "sentim_pos_thr = 0.05  \n",
    "# VADER score <= -0.05 is considered negative\n",
    "sentim_neg_thr = -0.05 \n",
    "# Derive len_thr_shrt from training data\n",
    "if train_messages_with_features:\n",
    "    df_train_feats = pd.DataFrame(train_messages_with_features)\n",
    "    #  5th percentile for \"too short\"\n",
    "    if 'message_length' in df_train_feats.columns and not df_train_feats['message_length'].empty:\n",
    "        len_thr_shrt = np.percentile(df_train_feats['message_length'], 2) \n",
    "        print(f\"Calculated Short Message Length Threshold (len_thr_shrt): {len_thr_shrt:.2f} characters\")\n",
    "    else:\n",
    "        print(\"'message_length' not found or empty in training features. Using default for len_thr_shrt.\")\n",
    "        len_thr_shrt = 10 # Default\n",
    "else:\n",
    "    print(\"Training features not available to determine length threshold. Using default for len_thr_shrt.\")\n",
    "    len_thr_shrt = 10 # Default\n",
    "\n",
    "if len_thr_shrt is None: # Fallback if percentile calculation failed for some reason\n",
    "    len_thr_shrt = 10 \n",
    "    print(f\"Using default Short Message Length Threshold (len_thr_shrt): {len_thr_shrt}\")\n",
    "\n",
    "\n",
    "print(f\"\\n--- Preparing to Process Test User Messages for Anomalies (Feature-Based) ---\")\n",
    "print(f\"Threshold for 'isAnomalousTooShort': Message length < {len_thr_shrt:.2f}\")\n",
    "print(f\"Thresholds for sentiment categories: Positive >= {sentim_pos_thr}, Negative <= {sentim_neg_thr}\")\n",
    "print(f\"Criteria for 'isAnomalousSentimentFlip': User sentiment category changes (positive <-> negative) from previous user message.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This list will store dictionaries, each representing a test user message with added anomaly flags\n",
    "test_message_data_anomaly = [] \n",
    "# This dictionary will track the last sentiment category of a user within each conversation\n",
    "conversation_last_user_sentiment_category = {}\n",
    "\n",
    "if  test_messages_with_features:\n",
    "    # Convert to DataFrame for easier sorting and iteration\n",
    "    df_test_feats = pd.DataFrame(test_messages_with_features)    \n",
    "    # Ensure correct data types and sort for sequential processing of sentiment flips\n",
    "    df_test_feats['timestamp'] = pd.to_datetime(df_test_feats['timestamp'])\n",
    "    df_test_feats = df_test_feats.sort_values(by=['conversation_id', 'timestamp'])\n",
    "\n",
    "    for index, msg_detail in df_test_feats.iterrows():\n",
    "        # Basic message properties\n",
    "        current_sentiment_score = msg_detail.get('sentiment_compound', 0)\n",
    "        message_length = msg_detail.get('message_length', float('inf'))\n",
    "        conv_id = msg_detail.get('conversation_id')\n",
    "        \n",
    "        # Anomaly Flag 1: Message is Too Short \n",
    "        isAnomalousTooShort = message_length < len_thr_shrt\n",
    "\n",
    "        # Anomaly Flag 2: Sentiment Flip (Positive <-> Negative) \n",
    "        isAnomalousSentimentFlip = False\n",
    "        \n",
    "        # Determine current message's sentiment category\n",
    "        current_sentiment_category = 'neutral'\n",
    "        if current_sentiment_score >= sentim_pos_thr:\n",
    "            current_sentiment_category = 'positive'\n",
    "        elif current_sentiment_score <= sentim_neg_thr:\n",
    "            current_sentiment_category = 'negative'\n",
    "\n",
    "        # Get the last known sentiment category for this user in this conversation\n",
    "        last_known_category_for_conv = conversation_last_user_sentiment_category.get(conv_id)\n",
    "         # If there was a previous user message from this user in this conversation\n",
    "        if last_known_category_for_conv:\n",
    "            # and the category has changed\n",
    "            if (last_known_category_for_conv == 'positive' and current_sentiment_category == 'negative') or \\\n",
    "                (last_known_category_for_conv == 'negative' and current_sentiment_category == 'positive'):\n",
    "                isAnomalousSentimentFlip = True\n",
    "        \n",
    "        # Update the last sentiment category for this user in this conversation\n",
    "        # This assumes msg_detail is a user message (which test_messages_with_features should contain)\n",
    "        conversation_last_user_sentiment_category[conv_id] = current_sentiment_category\n",
    "        \n",
    "        # Append to the list that will form the DataFrame\n",
    "        test_message_data_anomaly.append({\n",
    "            'conversation_id': conv_id,\n",
    "            'original_user_id': msg_detail.get('original_user_id'),\n",
    "            'message_text': msg_detail.get('message_text'),\n",
    "            'sentiment_compound': current_sentiment_score,\n",
    "            'message_length': message_length,\n",
    "            'user_sentiment_shift': msg_detail.get('user_sentiment_shift', 0.0), # Include this from previous feature engineering\n",
    "            'current_sentiment_category': current_sentiment_category,\n",
    "            'previous_user_sentiment_category_in_conv': last_known_category_for_conv if last_known_category_for_conv else 'N/A',\n",
    "            'isAnomalousTooShort': 1 if isAnomalousTooShort else 0,\n",
    "            'isAnomalousSentimentFlip': 1 if isAnomalousSentimentFlip else 0\n",
    "        })\n",
    "    \n",
    "    print(f\"\\nProcessed {len(test_message_data_anomaly)} test user messages and added anomaly flags.\")\n",
    "    if test_message_data_anomaly:\n",
    "            print(\"First processed message data structure (example):\")\n",
    "            print(test_message_data_anomaly[0])\n",
    "else:\n",
    "    print(\"The DataFrame created from 'test_messages_with_features' is empty.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Anomaly Analysis Results to CSV\n",
    "\n",
    "- Convert the processed test message data (with anomaly flags) to a DataFrame.\n",
    "- Create an overall anomaly flag (`isOverallAnomaly`) for each message (set if either anomaly condition is met).\n",
    "- Export the DataFrame to a CSV file for further analysis or reporting.\n",
    "- Identify and count test conversations that contain at least one anomalous message.\n",
    "- Print the number of conversations flagged and confirm the CSV export.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flagged_anomalous_conversations = []\n",
    "if test_message_data_anomaly and isinstance(test_message_data_anomaly, list):\n",
    "    # Convert processed message data to DataFrame\n",
    "    df_test_message_analysis = pd.DataFrame(test_message_data_anomaly)\n",
    "\n",
    "    if not df_test_message_analysis.empty:\n",
    "        # Create 'isOverallAnomaly' column (OR logic: message is too short OR has a sentiment flip)        \n",
    "        df_test_message_analysis['isOverallAnomaly'] = (\n",
    "            df_test_message_analysis['isAnomalousTooShort'] | df_test_message_analysis['isAnomalousSentimentFlip']\n",
    "        ).astype(int)\n",
    "\n",
    "        print(f\"Created DataFrame 'df_test_message_analysis' with shape: {df_test_message_analysis.shape}\")\n",
    "        print(f\"Number of messages flagged as 'isOverallAnomaly': {df_test_message_analysis['isOverallAnomaly'].sum()}\")\n",
    "\n",
    "        # Export DataFrame to CSV\n",
    "        csv_file_name = \"test_message_anomaly_analysis.csv\"\n",
    "        try:\n",
    "            df_test_message_analysis.to_csv(csv_file_name, index=False)\n",
    "            print(f\"DataFrame 'df_test_message_analysis' exported to '{csv_file_name}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error exporting DataFrame to CSV: {e}\")\n",
    "\n",
    "        # Identify Test Conversations Containing any 'isOverallAnomaly' Messages\n",
    "        if test_conversations and isinstance(test_conversations, list):\n",
    "            conv_ids_with_overall_anomaly = set(\n",
    "                df_test_message_analysis[df_test_message_analysis['isOverallAnomaly'] == 1]['conversation_id'].unique()\n",
    "            )\n",
    "\n",
    "            if conv_ids_with_overall_anomaly:\n",
    "                for conv_obj in test_conversations:\n",
    "                    if isinstance(conv_obj, dict) and conv_obj.get('ref_conversation_id') in conv_ids_with_overall_anomaly:\n",
    "                        flagged_anomalous_conversations.append(conv_obj)\n",
    "                \n",
    "                print(f\"Identified {len(flagged_anomalous_conversations)} test conversations containing overall anomalies.\")              \n",
    "            else:\n",
    "                print(\"No conversations in the test set were flagged based on the 'isOverallAnomaly' criterion.\")\n",
    "        else:\n",
    "            print(\"Original 'test_conversations' list not found or empty. Cannot identify anomalous conversations.\")            \n",
    "    else:\n",
    "        print(\"DataFrame 'df_test_message_analysis' created from 'test_message_data_anomaly' is empty.\")\n",
    "else:\n",
    "    print(\"'test_message_data_anomaly' not found, empty, or not a list. Cannot create analysis DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmRun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
