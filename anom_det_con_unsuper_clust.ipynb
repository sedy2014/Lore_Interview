{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Method1 : using embeddings and Clusterings \n",
    "Imagine a platform that processes a continuous stream of conversations from a \n",
    "generative agent (StoryBot). To maintain a safe and engaging environment, you need to monitor these conversations in real time for anomalies such as sudden shifts in sentiment, unusual topic spikes, or atypical user behavior that could indicate emerging issues or opportunities. \n",
    "**Task**\n",
    "-  Develop a streaming machine learning pipeline in Python that ingests data (message \n",
    "and metadata), does feature extraction and preprocessing in near real time.\n",
    "-   applies an anomaly detection model (your own or from a 3rd party), and returns alerts when anomalies are detected. \n",
    "- For this task, feel free to pick an anomaly you think would be interesting to track (e.g., changes in mood, change in emoji use, shifts in language, use of the phrase “I don’t know”,  prompt injection attacks, etc.).\n",
    "- Provide a brief README that outlines the architecture, setup instructions, and how to run tests.   \n",
    "The datasets include:\n",
    "\n",
    "**Conversations** - One-on-one conversations between users and an AI agent (StoryBot)\n",
    "### conversations.json\n",
    "\n",
    "Contains conversations between individual users and StoryBot. Each conversation includes:\n",
    "\n",
    "- `messages_list`: List of messages in the conversation\n",
    "- `ref_conversation_id`: Unique identifier for the conversation\n",
    "- `ref_user_id`: ID of the user participating in the conversation\n",
    "\n",
    "Example structure:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"messages_list\": [\n",
    "    {\"message\": \"Good morning! How are you feeling today?\",\n",
    "    \"ref_conversation_id\": 42615,\n",
    "    \"ref_user_id\": 1,\n",
    "    \"transaction_datetime_utc\":  \"2023-10-01T08:00:00Z\",\n",
    "    \"screen_name\": \"StoryBot\"\n",
    "    },\n",
    "    {\"message\": \"I'm doing well, thanks for asking! Just trying to get through the day.\",\n",
    "    \"ref_conversation_id\": 42615,\n",
    "    \"ref_user_id\": 822,\n",
    "    \"transaction_datetime_utc\":  \"2023-10-01T08:01:00Z\",\n",
    "    \"screen_name\": \"User822\"\n",
    "    },\n",
    "    {\"message\": \"That's great to hear! Is there anything specific on your mind?\",\n",
    "    \"ref_conversation_id\": 42615,\n",
    "    \"ref_user_id\": 1,\n",
    "    \"transaction_datetime_utc\":  \"2023-10-01T08:02:00Z\",\n",
    "    \"screen_name\": \"StoryBot\"\n",
    "    },\n",
    "  ],\n",
    "  \"ref_conversation_id\": 42615,\n",
    "  \"ref_user_id\": 822\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libarries\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read JSON File to Load Conversations\n",
    "\n",
    "- Define the folder and file paths for the dataset.\n",
    "- Prepare to load the `conversations.json` file from the `data` directory within the current working directory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get folder and filepaths\n",
    "#\n",
    "curr_folder = os.getcwd()\n",
    "conver_json = \"conversations.json\"\n",
    "json_file = os.path.join(curr_folder,\"data\",conver_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Conversations from JSON\n",
    "\n",
    "- Define a function `read_json()` to load conversation data from the specified JSON file.\n",
    "    - Handles file not found and JSON decoding errors gracefully.\n",
    "- Read the raw conversation data into memory.\n",
    "- Count and print the total number of conversations loaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file_path):\n",
    "    \"\"\"\n",
    "    Loads conversations from the conversations.json file.\n",
    "    Args:\n",
    "        file_path (str): The path to the JSON file.\n",
    "    Returns:\n",
    "        list: A list of conversation objects, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_file, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found: {file_path}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Could not decode JSON from the file {file_path}.\")\n",
    "        return None\n",
    "# read RAW JSON data\n",
    "raw_conversations = read_json(json_file)\n",
    "total_conv = len(raw_conversations)\n",
    "print(f\"number of conversations is {total_conv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split Data into Training and Test Sets\n",
    "\n",
    "- Use `train_test_split` from scikit-learn to divide the loaded conversations into training and test sets.\n",
    "- Set a random seed for reproducibility.\n",
    "- Define the percentage split between training and testing (default: 80% train, 20% test).\n",
    "- If the dataset is very small, warn the user and suggest using a K-fold strategy.\n",
    "- Print the number of conversations in each split.  \n",
    "\n",
    "**NOTE**: Alternatively, this could all be training set, and we test on a different set of conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # For splitting data\n",
    "import random\n",
    "\n",
    "# Assuming StoryBot's ID is 1, based on given json file \n",
    "STORYBOT_USER_ID = 1 \n",
    "\n",
    "RANDOM_SEED = 42 # For reproducible splits\n",
    "train_per = 80\n",
    "test_per = 100 - train_per\n",
    "min_conv_to_train = 8\n",
    "if total_conv > 0:\n",
    "    # Too small a data to train\n",
    "    if total_conv < min_conv_to_train: \n",
    "        print(\"Warning: Very few conversations. Use K-fold strategy to train and test\")        \n",
    "    else:\n",
    "        train_conversations, test_conversations = train_test_split(\n",
    "            raw_conversations, \n",
    "            test_size= test_per/100,       \n",
    "            random_state=RANDOM_SEED # For reproducibility\n",
    "        )\n",
    "\n",
    "    print(f\"\\nNumber of conversations for training: {len(train_conversations)}\")\n",
    "    print(f\"Number of conversations for testing: {len(test_conversations)}\")   \n",
    "else:\n",
    "    print(\"Cannot proceed with splitting as no conversations were loaded.\")\n",
    "    train_conversations, test_conversations = [], [] # Initialize as empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Function to Extract User Message Details\n",
    "\n",
    "- Define `extract_user_message_details()` to:\n",
    "    - Iterate through each conversation and its messages.\n",
    "    - Extract details for each message not sent by StoryBot, including:\n",
    "        - Conversation ID\n",
    "        - User ID\n",
    "        - Screen name\n",
    "        - Timestamp (parsed as datetime)\n",
    "        - Message text\n",
    "    - Handle malformed message data gracefully.\n",
    "- Returns a list of dictionaries, each representing a user message with relevant metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_user_message_details(list_of_conversations, is_storybot_user_id):\n",
    "    \"\"\"\n",
    "    Extracts message details (text, conv_id, timestamp) for all messages for all users\n",
    "    NOT sent by the is_storybot_user_id.\n",
    "\n",
    "    Args:\n",
    "        list_of_conversations (list): A list of conversation objects.\n",
    "        is_storybot_user_id (int): The user ID of StoryBot.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dict contains \n",
    "              'conversation_id', 'original_user_id', 'screen_name', \n",
    "              'timestamp', and 'message_text' for a user message.\n",
    "    \"\"\"\n",
    "    user_messages_details = []\n",
    "    # for each conversation with  a user\n",
    "    for conversation in list_of_conversations:\n",
    "        # exgtract conversation id\n",
    "        conv_id = conversation.get('ref_conversation_id', 'unknown_conv_id')\n",
    "        # extract list of  messages\n",
    "        messages_list = conversation.get('messages_list', [])\n",
    "        # build new list with updated datetime\n",
    "        for message_data in messages_list:\n",
    "            if not isinstance(message_data, dict): # Basic check for message structure\n",
    "                # print(f\"Skipping malformed message in conv {conv_id}\")\n",
    "                continue\n",
    "\n",
    "            message_user_id = message_data.get('ref_user_id')\n",
    "            \n",
    "            if message_user_id is not None and message_user_id != is_storybot_user_id:\n",
    "                try:\n",
    "                    timestamp_str = message_data.get('transaction_datetime_utc')\n",
    "                    parsed_timestamp = None\n",
    "                    if timestamp_str:\n",
    "                        parsed_timestamp = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))\n",
    "                    \n",
    "                    user_messages_details.append({\n",
    "                        'conversation_id': conv_id,\n",
    "                        'original_user_id': message_user_id,\n",
    "                        'screen_name': message_data.get('screen_name', 'UnknownUser'),\n",
    "                        'timestamp': parsed_timestamp,\n",
    "                        'message_text': message_data.get('message', '') # Ensure text is a string\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing a user message in conv {conv_id}: {e}. Message data: {message_data}\")\n",
    "\n",
    "    return user_messages_details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract Message Data for Training and Test\n",
    "\n",
    "- Use the previously defined function to extract user messages from both training and test sets.\n",
    "- Print the number of user messages extracted for each set.\n",
    "- Display an example message for sanity checking.\n",
    "- These extracted messages will be used for embedding and downstream anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract user messages for training\n",
    "train_user_message_details = []\n",
    "if train_conversations:\n",
    "    # create list of all training user data , with updated time\n",
    "    train_user_message_details = extract_user_message_details(train_conversations, STORYBOT_USER_ID)\n",
    "    print(f\"\\nExtracted {len(train_user_message_details)} user messages for training.\")\n",
    "    # print first  user message for sanity check\n",
    "    if train_user_message_details:\n",
    "        print(f\"Example training user message detail: {train_user_message_details[0]}\")\n",
    "\n",
    "# create list of all test user data , with updated time\n",
    "test_user_message_details = []\n",
    "if test_conversations:\n",
    "    test_user_message_details = extract_user_message_details(test_conversations, STORYBOT_USER_ID)\n",
    "    print(f\"Extracted {len(test_user_message_details)} user messages for testing.\")\n",
    "    if test_user_message_details:\n",
    "        print(f\"Example testing user message detail: {test_user_message_details[0]}\")\n",
    "\n",
    "# exytract  'message_text' from these lists.\n",
    "\n",
    "train_user_texts = [detail['message_text'] for detail in train_user_message_details]\n",
    "test_user_texts = [detail['message_text'] for detail in test_user_message_details]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create embeddings: which model to use\n",
    "-  Sentence Transformers :specifically designed to make generating high-quality sentence and text embeddings very straightforward\n",
    "-  Hugging Face Transformers : massive number of pre-trained Transformer models like BERT, RoBERTa, DistilBERT (need to handle tokenization and pooling myself)\n",
    "So, decided with Sentence tranformer.\n",
    "Now, I have various models to choose based on :  \n",
    "1   Size  \n",
    "2   Speed  \n",
    "3   Quality   \n",
    "4   paraphrasing, semantic similaity sensitivity  \n",
    "Starting with `all-MiniLM-L6-v2`, but having options for `all-mpnet-base-v2` and `paraphrase-MiniLM-L6-v2`\n",
    "\n",
    "##### Define Function to Create Embeddings\n",
    "\n",
    "- Define `create_embeddings()` to generate embeddings for a list of texts using a pre-loaded Sentence Transformer model.\n",
    "- Handles empty input gracefully.\n",
    "- Returns a NumPy array containing the embeddings.\n",
    "- Prints the number of sentences encoded and the resulting embedding shape.\n",
    "\n",
    "**NOTE** This approach is called **message-level anomaly detection**. I  decided on this because with only 24 total conversations (19 for training), trying to create a single embedding for an entire conversation (and then clustering only 19 such conversation-level embeddings) would likely be less stable and less effective than working with the larger number of individual user messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def create_embeddings(texts_to_encode, loaded_model):\n",
    "    \"\"\"\n",
    "    Calculates embeddings for a list of texts using a pre-loaded Sentence Transformer model.\n",
    "\n",
    "    Args:\n",
    "        texts_to_encode (list of str): The list of sentences to encode.\n",
    "        loaded_model (SentenceTransformer): The pre-loaded Sentence Transformer model.        \n",
    "    Returns:\n",
    "        numpy.ndarray: A NumPy array containing the embeddings.\n",
    "    \"\"\"\n",
    "    if not texts_to_encode: # Handle empty input\n",
    "        print(\"Input sentences list is empty. Returning an empty array.\")        \n",
    "        return np.array([]).reshape(0, loaded_model.get_sentence_embedding_dimension() if hasattr(loaded_model, 'get_sentence_embedding_dimension') else 384) # Assuming 384 for MiniLM if not found\n",
    "\n",
    "    print(f\"Encoding {len(texts_to_encode)} sentences...\")\n",
    "    embeddings = loaded_model.encode(texts_to_encode)\n",
    "    print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate Embeddings for Training and Test Messages\n",
    "\n",
    "- Load the selected Sentence Transformer model.\n",
    "- Generate embeddings for both training and test user messages.\n",
    "- Print confirmation messages and embedding shapes for verification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### call embedding creation\n",
    "sen_Tran_model_names = [\"all-MiniLM-L6-v2\",\"all-mpnet-base-v2\",\"paraphrase-MiniLM-L6-v2\"]\n",
    "# Load the model ONCE\n",
    "selected_model_name = sen_Tran_model_names[0]\n",
    "print(f\"Loading Sentence Transformer model: {selected_model_name}....\")\n",
    "sentence_model = SentenceTransformer(selected_model_name)\n",
    "print(\"Model loaded Sucessffuly.\")\n",
    "# training embeddings\n",
    "tr_embed = create_embeddings(train_user_texts,sentence_model)\n",
    "# test  embeddings\n",
    "tst_embed = create_embeddings(test_user_texts,sentence_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dimensionality Reduction: Why Use PCA?\n",
    "\n",
    "- Considered using UMAP and KMeans for dimensionality reduction and clustering, but decided against UMAP because:\n",
    "    - UMAP may distort distances or remove subtle semantic features important for anomaly detection.\n",
    "    - Anomalies found in a lower-dimensional space may not correspond to real anomalies in the original embedding space.\n",
    "- Instead, use PCA for dimensionality reduction to retain as much variance as possible while reducing dimensions.\n",
    "\n",
    "##### Apply PCA to Embeddings\n",
    "\n",
    "- Use PCA to reduce the dimensionality of the message embeddings.\n",
    "    - Retain 90% of the variance (or set a specific number of components).\n",
    "- Standardize embeddings before applying PCA for better performance.\n",
    "- Fit PCA on the training embeddings and transform both training and test embeddings.\n",
    "- Print explained variance ratio, number of components selected, and new embedding shapes.\n",
    "- Warn if there is a dimension mismatch between train and test embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    " \n",
    "# Choose 80% of the variance.\n",
    "n_components_pca = 0.9\n",
    "random_state_pca = 42 # For reproducibility\n",
    "\n",
    "print(f\"\\nOriginal training embedding shape: {tr_embed.shape}\")\n",
    "\n",
    "# Initialize PCA\n",
    "# If n_components is a float (0 to 1.0), it's the variance ratio to keep.\n",
    "# If it's an int, it's the number of components.\n",
    "pca_reducer = PCA(\n",
    "    n_components=n_components_pca,\n",
    "    random_state=random_state_pca\n",
    ")\n",
    "print(f\"Fitting PCA on training embeddings to retain {n_components_pca*100 if isinstance(n_components_pca, float) else n_components_pca} components/variance...\")\n",
    "\n",
    "# AppLy standard scaler first to training embeddings\n",
    "scaler = StandardScaler()\n",
    "scaled_tr_embed = scaler.fit_transform(tr_embed)\n",
    "# apply same scaling to Test embeddings\n",
    "scaled_tst_embed = scaler.transform(tst_embed) \n",
    "\n",
    "\n",
    "\n",
    "# apply PCA on train embeddings\n",
    "pca_tr_embeddings = pca_reducer.fit_transform(scaled_tr_embed)\n",
    "print(f\"Explained variance ratio by chosen components: {np.sum(pca_reducer.explained_variance_ratio_):.4f}\")\n",
    "print(f\"Number of components selected by PCA: {pca_reducer.n_components_}\")\n",
    "print(f\"Reduced training embedding shape: {pca_tr_embeddings.shape}\")\n",
    "\n",
    "# Transform the test embeddings using the *fitted* PCA reducer\n",
    "pca_tst_embeddings = None\n",
    "if tst_embed.shape[1] == tr_embed.shape[1]: \n",
    "    pca_tst_embeddings = pca_reducer.transform(scaled_tst_embed)\n",
    "    print(f\"Reduced test embedding shape: {pca_tst_embeddings.shape}\")\n",
    "else:\n",
    "    print(f\"Warning: Test embeddings feature dimension ({tst_embed.shape[1]}) does not match training ({tr_embed.shape[1]}). Skipping PCA transform for test data.\")\n",
    "    pca_tst_embeddings = np.array([]) # Ensure it's an empty array for later checks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create K-Means Clusters for Training Embeddings\n",
    "\n",
    "- Use K-Means clustering to group similar message embeddings in the reduced PCA space.\n",
    "- Test a range of cluster counts (k) from 2 to 15.\n",
    "- For each k:\n",
    "    - Fit K-Means and assign cluster labels.\n",
    "    - Calculate Within-Cluster Sum of Squares (WCSS) for the Elbow Method.\n",
    "    - Calculate the Silhouette Score for clustering quality.\n",
    "- Plot WCSS and Silhouette Scores to help select the optimal number of clusters.\n",
    "- Choose the optimal k based on these plots and print the choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score \n",
    "k_range = range(2, 16) \n",
    "wcss_scores = []\n",
    "silhouette_scores = []\n",
    "random_state_kmeans = 42 \n",
    "for k_val in k_range:\n",
    "    kmeans_temp = KMeans(\n",
    "        n_clusters=k_val,\n",
    "        random_state=random_state_kmeans,\n",
    "        n_init=10  # Explicitly set n_init to avoid warnings and ensure multiple runs\n",
    "    )\n",
    "    cluster_labels_temp = kmeans_temp.fit_predict(pca_tr_embeddings)\n",
    "    \n",
    "    # 1. WCSS (Inertia) for Elbow Method\n",
    "    wcss_scores.append(kmeans_temp.inertia_)\n",
    "    \n",
    "    # 2. Silhouette Score\n",
    "    #    Ensure there's more than 1 unique label to calculate silhouette score\n",
    "    if len(np.unique(cluster_labels_temp)) > 1:\n",
    "        score = silhouette_score(pca_tr_embeddings, cluster_labels_temp)\n",
    "        silhouette_scores.append(score)\n",
    "    else:\n",
    "        silhouette_scores.append(-1) # Or some other indicator of invalid score for this k\n",
    "    \n",
    "    print(f\"  For k={k_val}, WCSS: {wcss_scores[-1]:.2f}, Silhouette Score: {silhouette_scores[-1]:.4f}\")\n",
    "\n",
    "# Plot Elbow Method (WCSS)\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_range, wcss_scores, marker='o', linestyle='--')\n",
    "plt.title('Elbow Method for Optimal k (WCSS)')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('WCSS (Inertia)')\n",
    "plt.xticks(list(k_range))\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Silhouette Scores\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(k_range, silhouette_scores, marker='o', linestyle='--')\n",
    "plt.title('Silhouette Scores for Optimal k')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Average Silhouette Score')\n",
    "plt.xticks(list(k_range))\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Guidance for Choosing k ---\")\n",
    "print(\"Elbow Method: Look for a point on the WCSS plot where the rate of decrease sharply changes (the 'elbow').\")\n",
    "print(\"Silhouette Score: Look for the k that gives the highest average Silhouette Score.\")\n",
    "optimal_k = 11\n",
    "print(f\"CHOOSING Optimal K={optimal_k}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fit K-Means and Predict Clusters for Test Embeddings\n",
    "\n",
    "- Initialize K-Means with the chosen optimal number of clusters.\n",
    "- Fit K-Means on the PCA-reduced training embeddings and assign cluster labels.\n",
    "- Obtain cluster centroids in the PCA-reduced space.\n",
    "- Predict cluster labels for the PCA-reduced test embeddings.\n",
    "- Print confirmation messages and handle dimension mismatches gracefully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize K-Means with the chosen optimal_k\n",
    "kmeans_model = KMeans(\n",
    "    n_clusters=optimal_k, # Use the k you determined from the plots\n",
    "    random_state=random_state_kmeans,\n",
    "    n_init=10\n",
    ")\n",
    "\n",
    "# Fit K-Means on the PCA-reduced training embeddings and get cluster labels for training data\n",
    "train_cluster_labels = kmeans_model.fit_predict(pca_tr_embeddings)\n",
    "print(f\"K-Means fitting complete with k={optimal_k}. Cluster labels assigned to training data.\")\n",
    "# Get the cluster centroids (these are in the PCA-reduced space)\n",
    "cluster_centroids = kmeans_model.cluster_centers_\n",
    "print(f\"Shape of cluster centroids: {cluster_centroids.shape}\") # Should be (optimal_k, n_components_from_PCA)\n",
    "\n",
    "# Predict cluster labels for the PCA-reduced test embeddings\n",
    "test_cluster_labels = np.array([]) # Initialize as empty\n",
    "if pca_tst_embeddings is not None and pca_tst_embeddings.size > 0:\n",
    "    if pca_tst_embeddings.shape[1] == pca_tr_embeddings.shape[1]: # Check if dimensions match\n",
    "        test_cluster_labels = kmeans_model.predict(pca_tst_embeddings)\n",
    "        print(f\"Cluster labels predicted for PCA-reduced test data.\")       \n",
    "    else:\n",
    "        print(f\"Warning: Dimension mismatch between PCA-reduced train ({pca_tr_embeddings.shape[1]}) and test ({pca_tst_embeddings.shape[1]}) embeddings. Cannot predict test labels.\")\n",
    "else:\n",
    "    print(\"No PCA-reduced test embeddings to predict clusters for\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Anomaly Threshold for Clustering\n",
    "\n",
    "- Define an anomaly threshold based on the distribution of distances from training points to their nearest cluster centroid.\n",
    "- Use the 95th percentile of these distances as the anomaly threshold.\n",
    "    - This means the top 5% most distant training points define what is considered \"anomalous.\"\n",
    "    - Note: Being in the top 5% does not necessarily mean a message is \"bad\"-it could simply be rare or unique.\n",
    "- For each test point, calculate its minimum distance to any cluster centroid.\n",
    "- Flag test points as anomalies if their distance exceeds the threshold.\n",
    "- Print the number of anomalies detected in the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Anomaly Detection on Test Set using Distance to Nearest KMeans Centroid ---\n",
    "anom_Threhold = 99 /100\n",
    " # Will hold min distance of each test point to any centroid\n",
    "min_distances_test = np.array([])   \n",
    "        \n",
    "\n",
    "# : Compute distances for each test point to all centroids \n",
    "if pca_tst_embeddings is not None and pca_tst_embeddings.size > 0 and cluster_centroids.size > 0:\n",
    "    if pca_tst_embeddings.shape[1] == cluster_centroids.shape[1]:\n",
    "        distances_to_all_centroids_test = kmeans_model.transform(pca_tst_embeddings)  # shape: (num_test_samples, num_clusters)\n",
    "        min_distances_test = np.min(distances_to_all_centroids_test, axis=1)         # min distance per test point\n",
    "        \n",
    "\n",
    "#  Compute threshold using training data distance\n",
    "if pca_tr_embeddings.size > 0 and cluster_centroids.size > 0:\n",
    "    distances_to_all_centroids_train = kmeans_model.transform(pca_tr_embeddings)\n",
    "    min_distances_train = np.min(distances_to_all_centroids_train, axis=1)\n",
    "\n",
    "    if min_distances_train.size > 0:\n",
    "        # Choose the 95th percentile of training distances as anomaly threshold\n",
    "        anomaly_distance_threshold = np.percentile(min_distances_train, anom_Threhold)\n",
    "        print(f\"Calculated anomaly distance threshold (95th percentile of train distances): {anomaly_distance_threshold:.4f}\")\n",
    "        #Flag test points as anomalies if distance > threshold \n",
    "        if min_distances_test.size > 0 and anomaly_distance_threshold is not None:\n",
    "            test_anomalies_flags = min_distances_test > anomaly_distance_threshold\n",
    "            if test_anomalies_flags.size > 0:                \n",
    "                print(f\"Number of anomalies detected in test set: {np.sum(test_anomalies_flags)} out of {len(test_anomalies_flags)}\")\n",
    "# for i, is_anomalous_flag in enumerate(test_anomalies_flags):\n",
    "#     if is_anomalous_flag:\n",
    "#     # Ensure the message detail has 'conversation_id'\n",
    "#         if i < len(test_user_message_details) and 'conversation_id' in test_user_message_details[i]:\n",
    "#             anomalous_message_conv_ids.add(test_user_message_details[i]['conversation_id'])\n",
    "#         else:\n",
    "#             print(f\"Warning: Missing 'conversation_id' or index out of bounds for anomalous message at index {i} in test_user_message_details.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmRun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
